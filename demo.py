import torch
import gradio as gr
import argparse
import types
from transformers import AutoModelForCausalLM, AutoTokenizer
from generation_utils import assisted_decoding

def parse_args():
    parser = argparse.ArgumentParser(description="Gradio demo for assisted decoding")
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to the main model checkpoint")
    parser.add_argument("--assistant_checkpoint", type=str, required=True, help="Path to the assistant model checkpoint")
    return parser.parse_args()

class AssistedDecodingModel:
    def __init__(self, checkpoint, assistant_checkpoint):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")

        print(f"Loading tokenizer from {checkpoint}...")
        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)

        print(f"Loading main model from {checkpoint}...")
        self.model = AutoModelForCausalLM.from_pretrained(checkpoint).to(self.device)

        print(f"Loading assistant model from {assistant_checkpoint}...")
        self.assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint).to(self.device)

        # Overwrite _assisted_decoding method
        self.model._assisted_decoding = types.MethodType(assisted_decoding, self.model)
        print("Models loaded successfully!")

    def generate(self, input_text, max_length=200):
        messages = [
            {'role': 'user', 'content': input_text},
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        outputs = self.model.generate(
            **inputs,
            assistant_model=self.assistant_model,
            return_dict_in_generate=True,
            max_length=max_length,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        output, token_dict = outputs.sequences, outputs.token_dict
        decoded_output = self.tokenizer.decode(output[0], skip_special_tokens=True)

        # Extract only the assistant's response (removing the prompt)
        # This approach might need adjustment based on your tokenizer's behavior
        final_output = decoded_output[len(self.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):].strip()

        highlighted_output = self.highlight_tokens(token_dict)

        return final_output, highlighted_output

    def highlight_tokens(self, token_dict):
        highlight_accept = lambda text: f"<span style='color: orange;'>{text}</span>"
        highlight_reject = lambda text: f"<span style='color: gray; text-decoration: line-through;'>{text}</span>"

        # Improved decode function that handles newlines correctly
        def decode(token):
            if len(token) == 0:
                return ""
            text = self.tokenizer.decode(token[0], skip_special_tokens=False)
            # Replace newlines with HTML break tags
            text = text.replace('\n', '<br>')
            return text

        output_text = ""
        for accept_tokens, reject_tokens, next_token in zip(
            token_dict["accept_tokens"], token_dict["reject_tokens"], token_dict["next_token"]):
            output_text += highlight_accept(decode(accept_tokens))
            output_text += highlight_reject(decode(reject_tokens))
            output_text += decode(next_token)

        # Wrap in a div with white-space: pre-wrap to preserve other whitespace
        output_text = f"<div style='white-space: pre-wrap;'>{output_text}</div>"

        return output_text

def create_demo(model):
    with gr.Blocks() as demo:
        gr.Markdown("# Assisted Decoding Demo")
        gr.Markdown("This demo demonstrates the assisted decoding feature using a main model and an assistant model.")

        # Add color coding explanation
        gr.Markdown("""
        **Color Coding:**
        - <span style='color: orange'>Orange text</span>: Tokens predicted by the assistant model
        - <span style='color: gray; text-decoration: line-through'>Gray strikethrough text</span>: Rejected predictions
        - <span style='color: black'>Black text</span>: Tokens generated by the main model
        """)

        with gr.Row():
            with gr.Column():
                input_text = gr.Textbox(label="Input Text", placeholder="Type your message here...", lines=5)
                max_length = gr.Slider(minimum=50, maximum=1000, value=200, step=50, label="Max Length")
                generate_btn = gr.Button("Generate")

            with gr.Column():
                output = gr.Textbox(label="Output", lines=10)
                highlighted_output = gr.HTML(label="Highlighted Generation Process")


        generate_btn.click(
            fn=model.generate,
            inputs=[input_text, max_length],
            outputs=[output, highlighted_output]
        )

    return demo

def main():
    args = parse_args()
    model = AssistedDecodingModel(args.checkpoint, args.assistant_checkpoint)
    demo = create_demo(model)
    demo.launch(share=True)

if __name__ == "__main__":
    main()

# python demo.py --checkpoint meta-llama/Meta-Llama-3.1-8B-Instruct --assistant_checkpoint meta-llama/Llama-3.2-1B-Instruct